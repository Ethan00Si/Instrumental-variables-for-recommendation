{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data from scratch.\n",
    "\n",
    "**Follow procedures below step by step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DownLoad the training set and validation set from [URL](https://msnews.github.io/).\n",
    "\n",
    "Please unzip the two downloaded files.\n",
    "\n",
    "The files are shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mMINDlarge_dev\u001b[m\u001b[m   \u001b[1m\u001b[36mMINDlarge_train\u001b[m\u001b[m pre.ipynb\n",
      "\n",
      "./MINDlarge_dev:\n",
      "__placeholder__        entity_embedding.vec   relation_embedding.vec\n",
      "behaviors.tsv          news.tsv\n",
      "\n",
      "./MINDlarge_train:\n",
      "__placeholder__        entity_embedding.vec   relation_embedding.vec\n",
      "behaviors.tsv          news.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev_news = './MINDlarge_dev/news.tsv'\n",
    "train_news = './MINDlarge_train/news.tsv'\n",
    "\n",
    "news1 = pd.read_csv(dev_news, sep='\\t', \n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'title_entities', 'abstract_entity'])\n",
    "news2 = pd.read_csv(train_news, sep='\\t',\n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'title_entities', 'abstract_entity'])\n",
    "\n",
    "news_all = pd.concat([news1,news2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173550, 8)\n",
      "(104151, 8)\n"
     ]
    }
   ],
   "source": [
    "print(news_all.shape)\n",
    "news_all.drop_duplicates(subset=['NewsID'],keep='last',inplace=True) #remove duplicate news\n",
    "print(news_all.shape)\n",
    "# There are 130,380 news in MIND which is reported in table 2 of our paper. \n",
    "# Only the 104,151 news in train and dev sets was used in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>URL</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N75236</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NewsID   Category      SubCategory  \\\n",
       "0  N88753  lifestyle  lifestyleroyals   \n",
       "1  N23144     health       weightloss   \n",
       "2  N86255     health          medical   \n",
       "3  N93187       news        newsworld   \n",
       "4  N75236     health           voices   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  Dispose of unwanted prescription drugs during ...   \n",
       "3  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "4  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2                                                NaN   \n",
       "3  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "4  I felt like I was a fraud, and being an NBA wi...   \n",
       "\n",
       "                                             URL  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "3  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "4  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                     abstract_entity  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2                                                 []  \n",
       "3  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "4  [{\"Label\": \"National Basketball Association\", ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter data in columns of the category and subcategory manually.\n",
    "\n",
    "Otherwise, pre-trained language models cannot recognize unseen tokens, e.g., \"lifestyle\" and \"lifestyleroyals\".\n",
    "\n",
    "For example, change \"lifestyle\" into \"life style\".\n",
    "\n",
    "We use vscode to find and replace all occurances of these untokinezed tokens.\n",
    "\n",
    "After filtering, please run codes above again.\n",
    "\n",
    "Here shows clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>URL</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>life style</td>\n",
       "      <td>life style royals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weight loss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NewsID    Category        SubCategory  \\\n",
       "0  N88753  life style  life style royals   \n",
       "1  N23144      health        weight loss   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "\n",
       "                                             URL  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "\n",
       "                                     abstract_entity  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "news_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One search query for each news article was created by concatenating the texts of its category, subcategory and the entities in the metadata. \n",
    "\n",
    "For a few number of articles where the entities are missing (both title entities and abstract entities are empty sets), ‚ÄúNLTK‚Äù  was used to extract entities from the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "            if type(i) == Tree:\n",
    "                    current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            if current_chunk:\n",
    "                    named_entity = \" \".join(current_chunk)\n",
    "                    if named_entity not in continuous_chunk:\n",
    "                            continuous_chunk.append(named_entity)\n",
    "                            current_chunk = []\n",
    "            else:\n",
    "                    continue\n",
    "    return continuous_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104151/104151 [10:16<00:00, 168.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "news_entity = news_all.copy()\n",
    "news_entity['total_entities'] = ''\n",
    "news = news_all.to_numpy()\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(news)), position=0, leave=True):\n",
    "    if news[idx][6] == '[]' and news[idx][7]=='[]':\n",
    "        if type(news[idx][4])==float:\n",
    "            '''news without abstract'''\n",
    "            tit_abs = news[idx][3]\n",
    "        else:\n",
    "            tit_abs = news[idx][3]+' '+news[idx][4]\n",
    "     \n",
    "        entity = get_continuous_chunks(tit_abs)\n",
    "        \n",
    "        total_entities = ' '.join(entity)\n",
    "    \n",
    "        news_entity.at[idx,'total_entities'] = total_entities\n",
    "    else:\n",
    "        total_entities = []\n",
    "        if news[idx][6] != '[]' and type(news[idx][6])!=float:\n",
    "            try:\n",
    "                total_entities.append([x['Label'] for x in eval(news[idx][6])])\n",
    "            except:\n",
    "                print(idx)\n",
    "        if news[idx][7] != '[]' and type(news[idx][7])!=float:\n",
    "            try:\n",
    "                total_entities.append([x['Label'] for x in eval(news[idx][7])])\n",
    "            except:\n",
    "                print(idx)\n",
    "        total_entities = [item for sub_list in total_entities for item in sub_list]\n",
    "        total_entities = ' '.join(str(e) for e in total_entities)\n",
    "        news_entity.at[idx,'total_entities'] = total_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>URL</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entity</th>\n",
       "      <th>total_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>life style</td>\n",
       "      <td>life style royals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Prince Philip, Duke of Edinburgh Charles, Prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weight loss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>Adipose tissue Adipose tissue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Drug Enforcement Administration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NewsID    Category        SubCategory  \\\n",
       "0  N88753  life style  life style royals   \n",
       "1  N23144      health        weight loss   \n",
       "2  N86255      health            medical   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  Dispose of unwanted prescription drugs during ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2                                                NaN   \n",
       "\n",
       "                                             URL  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "\n",
       "                                     abstract_entity  \\\n",
       "0                                                 []   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                 []   \n",
       "\n",
       "                                      total_entities  \n",
       "0  Prince Philip, Duke of Edinburgh Charles, Prin...  \n",
       "1                      Adipose tissue Adipose tissue  \n",
       "2                    Drug Enforcement Administration  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_entity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store intermdediate results\n",
    "news_entity = news_entity.dropna(subset=['NewsID'])\n",
    "news_entity.to_csv('news_entity_.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104151/104151 [00:17<00:00, 6042.59it/s]\n"
     ]
    }
   ],
   "source": [
    "'''gather texts into groups(query and item)'''\n",
    "from tqdm import tqdm\n",
    "news = news_entity.copy()\n",
    "news['query'] = ''  # query text\n",
    "news['embedding']='' # item text \n",
    "news = news.drop(['URL','Category','SubCategory','Title','Abstract','total_entities','title_entities','abstract_entity'],axis=1)\n",
    "# news = news.to_numpy()\n",
    "news_entity = news_entity.fillna('')\n",
    "\n",
    "for idx in tqdm(range(len(news))):\n",
    "    news.loc[idx,'query'] = news_entity.loc[idx,'Category']+' '+news_entity.loc[idx,'SubCategory']+' '+news_entity.loc[idx,'total_entities']\n",
    "    news.loc[idx,'embedding'] = news_entity.loc[idx,'Title']+' '+news_entity.loc[idx,'Abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104151, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsID</th>\n",
       "      <th>query</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>life style life style royals Prince Philip, Du...</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health weight loss Adipose tissue Adipose tissue</td>\n",
       "      <td>50 Worst Habits For Belly Fat These seemingly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health medical Drug Enforcement Administration</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NewsID                                              query  \\\n",
       "0  N88753  life style life style royals Prince Philip, Du...   \n",
       "1  N23144   health weight loss Adipose tissue Adipose tissue   \n",
       "2  N86255     health medical Drug Enforcement Administration   \n",
       "\n",
       "                                           embedding  \n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...  \n",
       "1  50 Worst Habits For Belly Fat These seemingly ...  \n",
       "2  Dispose of unwanted prescription drugs during ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('news.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the line number as index for each news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ID2idx = dict()\n",
    "\n",
    "cnt = 1\n",
    "# index starts from 1\n",
    "# 0 for padding\n",
    "for idx,line in news.iterrows():\n",
    "    if ID2idx.__contains__(line[0]) == False:\n",
    "        ID2idx[line[0]] = cnt\n",
    "        cnt += 1\n",
    "\n",
    "json_str = json.dumps(ID2idx)\n",
    "with open('ID2idx.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we process the user-item interaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|2186682/2186682 [32:26<00:00, 1103.29it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'MINDlarge_train'\n",
    "behaviors = pd.read_csv(folder_name+\"/behaviors.tsv\",sep='\\t')\n",
    "behaviors = behaviors.dropna(axis=0) #drop users without history\n",
    "behaviors = behaviors.to_numpy()\n",
    "\n",
    "\n",
    "import json\n",
    "ID2idx = open('ID2idx.json','r')\n",
    "ID2idx = json.load(ID2idx)\n",
    "\n",
    "padding_length = 50 # truncate user browsing history at 50\n",
    "\n",
    "fout = open('./train.csv','w')\n",
    "\n",
    "for line in tqdm(behaviors):\n",
    "    user = line[3].split(' ')\n",
    "    user = [ID2idx[x] for x in user]\n",
    "    if len(user)>padding_length:\n",
    "        user = user[:padding_length]\n",
    "    else:\n",
    "        for i in range(padding_length-len(user)):\n",
    "            user.append(0)\n",
    "    iteract_items = line[4].split(' ')\n",
    "    iteract_items = [item.split('-') for item in iteract_items]\n",
    "    for iteract in iteract_items:\n",
    "        # fout.write(str(user))\n",
    "        for item in user:\n",
    "            fout.write(str(item)+',')\n",
    "        # fout.write(',')\n",
    "        fout.write(str(ID2idx[iteract[0]]))\n",
    "        fout.write(',')\n",
    "        fout.write(str(iteract[1]))\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,86996,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,92955,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,66393,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,71655,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,87166,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,29887,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,95828,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,68380,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91271,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89921,1\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,71811,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,102327,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72013,1\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,102440,0\n",
      "16340,49602,39636,18665,23724,28793,872,59097,35523,35775,10698,40145,50154,36619,54478,54645,41267,39983,29231,38994,57195,71239,65272,71796,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89009,0\n"
     ]
    }
   ],
   "source": [
    "#show training dataset\n",
    "!head -n 15 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 365200/365200 [04:24<00:01, 1081.76it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'MINDlarge_dev'\n",
    "behaviors = pd.read_csv(folder_name+\"/behaviors.tsv\",sep='\\t')\n",
    "behaviors = behaviors.dropna(axis=0) #drop users without history\n",
    "behaviors = behaviors.to_numpy()\n",
    "\n",
    "\n",
    "import json\n",
    "ID2idx = open('ID2idx.json','r')\n",
    "ID2idx = json.load(ID2idx)\n",
    "\n",
    "padding_length = 50 # truncate user browsing history at 50\n",
    "\n",
    "fout = open('./dev.csv','w')\n",
    "# fout.write('user,item,ctr\\n')\n",
    "for line in tqdm(behaviors):\n",
    "    user = line[3].split(' ')\n",
    "    user = [ID2idx[x] for x in user]\n",
    "    if len(user)>padding_length:\n",
    "        user = user[:padding_length]\n",
    "    else:\n",
    "        for i in range(padding_length-len(user)):\n",
    "            user.append(0)\n",
    "    iteract_items = line[4].split(' ')\n",
    "    iteract_items = [item.split('-') for item in iteract_items]\n",
    "    for iteract in iteract_items:\n",
    "        # fout.write(str(user))\n",
    "        for item in user:\n",
    "            fout.write(str(item)+',')\n",
    "        # fout.write(',')\n",
    "        fout.write(str(ID2idx[iteract[0]]))\n",
    "        fout.write(',')\n",
    "        fout.write(str(iteract[1]))\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,70258,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,71795,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,70216,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,62883,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,63729,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,62066,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,69153,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,65398,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,67964,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,64741,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,67999,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,34766,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,63425,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,63192,0\n",
      "62326,56055,50011,56063,14115,18785,39544,19156,27416,52335,8319,14915,940,872,18691,23304,45940,49597,56185,13217,21563,58333,35183,9354,30137,44644,47782,55564,12334,34692,53057,49667,38773,5266,27909,53101,46648,27154,41911,37117,3043,45472,47277,29900,56863,46850,56989,34069,30705,45279,1142,0\n"
     ]
    }
   ],
   "source": [
    "#show test dataset\n",
    "!head -n 15 dev.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we embed news and queries into vectors using a pre-trained language model.\n",
    "\n",
    "We use the pre-trained model provided by huggingface transformer.\n",
    "\n",
    "The next two code blocks cost several hours to run. We suggest you to use GPU to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torchsnooper\n",
    "\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',do_lower_case=True)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "import pandas as pd\n",
    "news = pd.read_csv('news.tsv',sep='\\t')\n",
    "embedding = news['embedding'].to_numpy() #generate item embedding\n",
    " \n",
    "import gc\n",
    "embedding_vec = torch.zeros((embedding.shape[0],768))\n",
    "batch_size = 8\n",
    "for i in tqdm(range(math.ceil(embedding.shape[0] / batch_size))):\n",
    "    batch_input = embedding[i*batch_size:(i+1)*batch_size].tolist()\n",
    "    tokens = tokenizer(batch_input,padding=\"max_length\",truncation=True,return_tensors=\"pt\",max_length=100)\n",
    "    tokens = tokens.to('cuda:0')\n",
    "    features = model(**tokens)\n",
    "    features = features.last_hidden_state[:,0]\n",
    "    embedding_vec[i*batch_size+1:(i+1)*batch_size+1] = features\n",
    "    del features\n",
    "    gc.collect()\n",
    "\n",
    "embedding_vec = embedding_vec.numpy()\n",
    "\n",
    "import numpy as np\n",
    "embedding_vec = np.vstack([np.array([0.0]*768), embedding_vec]) #padding\n",
    "np.save('embedding_vec.npy', embedding_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torchsnooper\n",
    "\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',do_lower_case=True)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "import pandas as pd\n",
    "news = pd.read_csv('news.tsv',sep='\\t')\n",
    "embedding = news['query'].to_numpy() #generate query embedding\n",
    " \n",
    "import gc\n",
    "embedding_vec = torch.zeros((embedding.shape[0],768))\n",
    "batch_size = 8\n",
    "for i in tqdm(range(math.ceil(embedding.shape[0] / batch_size))):\n",
    "    batch_input = embedding[i*batch_size:(i+1)*batch_size].tolist()\n",
    "    tokens = tokenizer(batch_input,padding=\"max_length\",truncation=True,return_tensors=\"pt\",max_length=100)\n",
    "    tokens = tokens.to('cuda:0')\n",
    "    features = model(**tokens)\n",
    "    features = features.last_hidden_state[:,0]\n",
    "    embedding_vec[i*batch_size+1:(i+1)*batch_size+1] = features\n",
    "    del features\n",
    "    gc.collect()\n",
    "\n",
    "embedding_vec = embedding_vec.numpy()\n",
    "\n",
    "import numpy as np\n",
    "embedding_vec = np.vstack([np.array([0.0]*768), embedding_vec]) #padding\n",
    "np.save('query_vec.npy', embedding_vec) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the MIND dataset, ùëÅ was set to 1 since only one query was created for each item (news article). $N$ is the number of queries used as IVs in Equation 2 in Section 4.2 of the paper.\n",
    "\n",
    "In our experiments, since the query embedding matrix is fixed, we can calculate pseudoinverse of corresponding query embedding matrix offline to accelerate training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "query = np.load(\"query_vec.npy\",allow_pickle=True)\n",
    "# query_vec.npy is Zt\n",
    "query = torch.Tensor(query)\n",
    "\n",
    "\n",
    "Zt = query.unsqueeze(-1)#np.zeros((query_index.shape[0], 64, top_k))\n",
    "Zt_pinv = np.zeros((query.shape[0],  1, 768))\n",
    "\n",
    "      \n",
    "for i in tqdm(range(Zt.shape[0])):\n",
    "    Zt_pinv[i] = np.linalg.pinv(Zt[i])\n",
    "\n",
    "np.save(\"./Zt_pinv.npy\", Zt_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6be698d5214e40b546197b862755e6fdc04613eaf8f125f64a3396a282195836"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('szh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
